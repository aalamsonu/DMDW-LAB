1.Read any CSV data file from google colab and create a dataframe using pandas. Next find mean value of each column. Find indices where non value is present. Replace the missing value using the mean value.
In [1]:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
In [2]:
car_sales = pd.read_csv("car-sales-extended-missing-data.csv")
In [3]:
car_sales
Out[3]:
Make	Colour	Odometer (KM)	Doors	Price
0	Honda	White	35431.0	4.0	15323.0
1	BMW	Blue	192714.0	5.0	19943.0
2	Honda	White	84714.0	4.0	28343.0
3	Toyota	White	154365.0	4.0	13434.0
4	Nissan	Blue	181577.0	3.0	14043.0
...	...	...	...	...	...
995	Toyota	Black	35820.0	4.0	32042.0
996	NaN	White	155144.0	3.0	5716.0
997	Nissan	Blue	66604.0	4.0	31570.0
998	Honda	White	215883.0	4.0	4001.0
999	Toyota	Blue	248360.0	4.0	12732.0
1000 rows × 5 columns

In [4]:
x = np.mean(car_sales["Odometer (KM)"])
x
Out[4]:
131253.23789473684
In [5]:
y = np.mean(car_sales["Doors"])
y
Out[5]:
4.011578947368421
In [6]:
z = np.mean(car_sales["Price"])
z
Out[6]:
16042.814736842105
In [7]:
car_sales.isnull().sum()
Out[7]:
Make             49
Colour           50
Odometer (KM)    50
Doors            50
Price            50
dtype: int64
In [9]:
car_sales.fillna(np.mean(car_sales["Odometer (KM)"]),inplace=True)
In [10]:
car_sales.isna().sum()
Out[10]:
Make             0
Colour           0
Odometer (KM)    0
Doors            0
Price            0
dtype: int64
2. Imagine you are given a list of items in a DataFrame as below.
D=["A","B","C","D","E","AA","AB"].Now convert and display this categorical data using dummies function of pandas and label encoder function of Scikit-learn.

In [11]:
# Create dataframe
df = pd.DataFrame({'D': ["A","B","C","D","E","AA","AB"],
'B': [10,20,30,40,50,60,70]}, index=[0,1,2,3,4,5,6])
print(df)
# using get_dummies function of pandas package
df_dummies= pd.get_dummies(df, prefix='A', 
columns=['D'])
print(df_dummies)
    D   B
0   A  10
1   B  20
2   C  30
3   D  40
4   E  50
5  AA  60
6  AB  70
    B  A_A  A_AA  A_AB  A_B  A_C  A_D  A_E
0  10    1     0     0    0    0    0    0
1  20    0     0     0    1    0    0    0
2  30    0     0     0    0    1    0    0
3  40    0     0     0    0    0    1    0
4  50    0     0     0    0    0    0    1
5  60    0     1     0    0    0    0    0
6  70    0     0     1    0    0    0    0
In [12]:
# Alternatively you can use sklearn package's LabelEncoder function
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['A_AB'] = le.fit_transform(df.D)
print(df)
    D   B  A_AB
0   A  10     0
1   B  20     3
2   C  30     4
3   D  40     5
4   E  50     6
5  AA  60     1
6  AB  70     2
3. Read a csv details using pandas dataframe then apply min-max normalization and z-score normalization over the each column of the dataset.¶
In [13]:
from sklearn import datasets
from sklearn import preprocessing
# Load dataset from sklearn dataset package
iris_ = datasets.load_iris()
# seperate input and target features
X = iris_.data
y = iris_.target
print(X)
#Min-max normalization
minmax_scale = preprocessing.MinMaxScaler().fit(X)
X_minmax = minmax_scale.transform(X)
print(X_minmax)
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]
 [5.4 3.9 1.7 0.4]
 [4.6 3.4 1.4 0.3]
 [5.  3.4 1.5 0.2]
 [4.4 2.9 1.4 0.2]
 [4.9 3.1 1.5 0.1]
 [5.4 3.7 1.5 0.2]
 [4.8 3.4 1.6 0.2]
 [4.8 3.  1.4 0.1]
 [4.3 3.  1.1 0.1]
 [5.8 4.  1.2 0.2]
 [5.7 4.4 1.5 0.4]
 [5.4 3.9 1.3 0.4]
 [5.1 3.5 1.4 0.3]
 [5.7 3.8 1.7 0.3]
 [5.1 3.8 1.5 0.3]
 [5.4 3.4 1.7 0.2]
 [5.1 3.7 1.5 0.4]
 [4.6 3.6 1.  0.2]
 [5.1 3.3 1.7 0.5]
 [4.8 3.4 1.9 0.2]
 [5.  3.  1.6 0.2]
 [5.  3.4 1.6 0.4]
 [5.2 3.5 1.5 0.2]
 [5.2 3.4 1.4 0.2]
 [4.7 3.2 1.6 0.2]
 [4.8 3.1 1.6 0.2]
 [5.4 3.4 1.5 0.4]
 [5.2 4.1 1.5 0.1]
 [5.5 4.2 1.4 0.2]
 [4.9 3.1 1.5 0.2]
 [5.  3.2 1.2 0.2]
 [5.5 3.5 1.3 0.2]
 [4.9 3.6 1.4 0.1]
 [4.4 3.  1.3 0.2]
 [5.1 3.4 1.5 0.2]
 [5.  3.5 1.3 0.3]
 [4.5 2.3 1.3 0.3]
 [4.4 3.2 1.3 0.2]
 [5.  3.5 1.6 0.6]
 [5.1 3.8 1.9 0.4]
 [4.8 3.  1.4 0.3]
 [5.1 3.8 1.6 0.2]
 [4.6 3.2 1.4 0.2]
 [5.3 3.7 1.5 0.2]
 [5.  3.3 1.4 0.2]
 [7.  3.2 4.7 1.4]
 [6.4 3.2 4.5 1.5]
 [6.9 3.1 4.9 1.5]
 [5.5 2.3 4.  1.3]
 [6.5 2.8 4.6 1.5]
 [5.7 2.8 4.5 1.3]
 [6.3 3.3 4.7 1.6]
 [4.9 2.4 3.3 1. ]
 [6.6 2.9 4.6 1.3]
 [5.2 2.7 3.9 1.4]
 [5.  2.  3.5 1. ]
 [5.9 3.  4.2 1.5]
 [6.  2.2 4.  1. ]
 [6.1 2.9 4.7 1.4]
 [5.6 2.9 3.6 1.3]
 [6.7 3.1 4.4 1.4]
 [5.6 3.  4.5 1.5]
 [5.8 2.7 4.1 1. ]
 [6.2 2.2 4.5 1.5]
 [5.6 2.5 3.9 1.1]
 [5.9 3.2 4.8 1.8]
 [6.1 2.8 4.  1.3]
 [6.3 2.5 4.9 1.5]
 [6.1 2.8 4.7 1.2]
 [6.4 2.9 4.3 1.3]
 [6.6 3.  4.4 1.4]
 [6.8 2.8 4.8 1.4]
 [6.7 3.  5.  1.7]
 [6.  2.9 4.5 1.5]
 [5.7 2.6 3.5 1. ]
 [5.5 2.4 3.8 1.1]
 [5.5 2.4 3.7 1. ]
 [5.8 2.7 3.9 1.2]
 [6.  2.7 5.1 1.6]
 [5.4 3.  4.5 1.5]
 [6.  3.4 4.5 1.6]
 [6.7 3.1 4.7 1.5]
 [6.3 2.3 4.4 1.3]
 [5.6 3.  4.1 1.3]
 [5.5 2.5 4.  1.3]
 [5.5 2.6 4.4 1.2]
 [6.1 3.  4.6 1.4]
 [5.8 2.6 4.  1.2]
 [5.  2.3 3.3 1. ]
 [5.6 2.7 4.2 1.3]
 [5.7 3.  4.2 1.2]
 [5.7 2.9 4.2 1.3]
 [6.2 2.9 4.3 1.3]
 [5.1 2.5 3.  1.1]
 [5.7 2.8 4.1 1.3]
 [6.3 3.3 6.  2.5]
 [5.8 2.7 5.1 1.9]
 [7.1 3.  5.9 2.1]
 [6.3 2.9 5.6 1.8]
 [6.5 3.  5.8 2.2]
 [7.6 3.  6.6 2.1]
 [4.9 2.5 4.5 1.7]
 [7.3 2.9 6.3 1.8]
 [6.7 2.5 5.8 1.8]
 [7.2 3.6 6.1 2.5]
 [6.5 3.2 5.1 2. ]
 [6.4 2.7 5.3 1.9]
 [6.8 3.  5.5 2.1]
 [5.7 2.5 5.  2. ]
 [5.8 2.8 5.1 2.4]
 [6.4 3.2 5.3 2.3]
 [6.5 3.  5.5 1.8]
 [7.7 3.8 6.7 2.2]
 [7.7 2.6 6.9 2.3]
 [6.  2.2 5.  1.5]
 [6.9 3.2 5.7 2.3]
 [5.6 2.8 4.9 2. ]
 [7.7 2.8 6.7 2. ]
 [6.3 2.7 4.9 1.8]
 [6.7 3.3 5.7 2.1]
 [7.2 3.2 6.  1.8]
 [6.2 2.8 4.8 1.8]
 [6.1 3.  4.9 1.8]
 [6.4 2.8 5.6 2.1]
 [7.2 3.  5.8 1.6]
 [7.4 2.8 6.1 1.9]
 [7.9 3.8 6.4 2. ]
 [6.4 2.8 5.6 2.2]
 [6.3 2.8 5.1 1.5]
 [6.1 2.6 5.6 1.4]
 [7.7 3.  6.1 2.3]
 [6.3 3.4 5.6 2.4]
 [6.4 3.1 5.5 1.8]
 [6.  3.  4.8 1.8]
 [6.9 3.1 5.4 2.1]
 [6.7 3.1 5.6 2.4]
 [6.9 3.1 5.1 2.3]
 [5.8 2.7 5.1 1.9]
 [6.8 3.2 5.9 2.3]
 [6.7 3.3 5.7 2.5]
 [6.7 3.  5.2 2.3]
 [6.3 2.5 5.  1.9]
 [6.5 3.  5.2 2. ]
 [6.2 3.4 5.4 2.3]
 [5.9 3.  5.1 1.8]]
[[0.22222222 0.625      0.06779661 0.04166667]
 [0.16666667 0.41666667 0.06779661 0.04166667]
 [0.11111111 0.5        0.05084746 0.04166667]
 [0.08333333 0.45833333 0.08474576 0.04166667]
 [0.19444444 0.66666667 0.06779661 0.04166667]
 [0.30555556 0.79166667 0.11864407 0.125     ]
 [0.08333333 0.58333333 0.06779661 0.08333333]
 [0.19444444 0.58333333 0.08474576 0.04166667]
 [0.02777778 0.375      0.06779661 0.04166667]
 [0.16666667 0.45833333 0.08474576 0.        ]
 [0.30555556 0.70833333 0.08474576 0.04166667]
 [0.13888889 0.58333333 0.10169492 0.04166667]
 [0.13888889 0.41666667 0.06779661 0.        ]
 [0.         0.41666667 0.01694915 0.        ]
 [0.41666667 0.83333333 0.03389831 0.04166667]
 [0.38888889 1.         0.08474576 0.125     ]
 [0.30555556 0.79166667 0.05084746 0.125     ]
 [0.22222222 0.625      0.06779661 0.08333333]
 [0.38888889 0.75       0.11864407 0.08333333]
 [0.22222222 0.75       0.08474576 0.08333333]
 [0.30555556 0.58333333 0.11864407 0.04166667]
 [0.22222222 0.70833333 0.08474576 0.125     ]
 [0.08333333 0.66666667 0.         0.04166667]
 [0.22222222 0.54166667 0.11864407 0.16666667]
 [0.13888889 0.58333333 0.15254237 0.04166667]
 [0.19444444 0.41666667 0.10169492 0.04166667]
 [0.19444444 0.58333333 0.10169492 0.125     ]
 [0.25       0.625      0.08474576 0.04166667]
 [0.25       0.58333333 0.06779661 0.04166667]
 [0.11111111 0.5        0.10169492 0.04166667]
 [0.13888889 0.45833333 0.10169492 0.04166667]
 [0.30555556 0.58333333 0.08474576 0.125     ]
 [0.25       0.875      0.08474576 0.        ]
 [0.33333333 0.91666667 0.06779661 0.04166667]
 [0.16666667 0.45833333 0.08474576 0.04166667]
 [0.19444444 0.5        0.03389831 0.04166667]
 [0.33333333 0.625      0.05084746 0.04166667]
 [0.16666667 0.66666667 0.06779661 0.        ]
 [0.02777778 0.41666667 0.05084746 0.04166667]
 [0.22222222 0.58333333 0.08474576 0.04166667]
 [0.19444444 0.625      0.05084746 0.08333333]
 [0.05555556 0.125      0.05084746 0.08333333]
 [0.02777778 0.5        0.05084746 0.04166667]
 [0.19444444 0.625      0.10169492 0.20833333]
 [0.22222222 0.75       0.15254237 0.125     ]
 [0.13888889 0.41666667 0.06779661 0.08333333]
 [0.22222222 0.75       0.10169492 0.04166667]
 [0.08333333 0.5        0.06779661 0.04166667]
 [0.27777778 0.70833333 0.08474576 0.04166667]
 [0.19444444 0.54166667 0.06779661 0.04166667]
 [0.75       0.5        0.62711864 0.54166667]
 [0.58333333 0.5        0.59322034 0.58333333]
 [0.72222222 0.45833333 0.66101695 0.58333333]
 [0.33333333 0.125      0.50847458 0.5       ]
 [0.61111111 0.33333333 0.61016949 0.58333333]
 [0.38888889 0.33333333 0.59322034 0.5       ]
 [0.55555556 0.54166667 0.62711864 0.625     ]
 [0.16666667 0.16666667 0.38983051 0.375     ]
 [0.63888889 0.375      0.61016949 0.5       ]
 [0.25       0.29166667 0.49152542 0.54166667]
 [0.19444444 0.         0.42372881 0.375     ]
 [0.44444444 0.41666667 0.54237288 0.58333333]
 [0.47222222 0.08333333 0.50847458 0.375     ]
 [0.5        0.375      0.62711864 0.54166667]
 [0.36111111 0.375      0.44067797 0.5       ]
 [0.66666667 0.45833333 0.57627119 0.54166667]
 [0.36111111 0.41666667 0.59322034 0.58333333]
 [0.41666667 0.29166667 0.52542373 0.375     ]
 [0.52777778 0.08333333 0.59322034 0.58333333]
 [0.36111111 0.20833333 0.49152542 0.41666667]
 [0.44444444 0.5        0.6440678  0.70833333]
 [0.5        0.33333333 0.50847458 0.5       ]
 [0.55555556 0.20833333 0.66101695 0.58333333]
 [0.5        0.33333333 0.62711864 0.45833333]
 [0.58333333 0.375      0.55932203 0.5       ]
 [0.63888889 0.41666667 0.57627119 0.54166667]
 [0.69444444 0.33333333 0.6440678  0.54166667]
 [0.66666667 0.41666667 0.6779661  0.66666667]
 [0.47222222 0.375      0.59322034 0.58333333]
 [0.38888889 0.25       0.42372881 0.375     ]
 [0.33333333 0.16666667 0.47457627 0.41666667]
 [0.33333333 0.16666667 0.45762712 0.375     ]
 [0.41666667 0.29166667 0.49152542 0.45833333]
 [0.47222222 0.29166667 0.69491525 0.625     ]
 [0.30555556 0.41666667 0.59322034 0.58333333]
 [0.47222222 0.58333333 0.59322034 0.625     ]
 [0.66666667 0.45833333 0.62711864 0.58333333]
 [0.55555556 0.125      0.57627119 0.5       ]
 [0.36111111 0.41666667 0.52542373 0.5       ]
 [0.33333333 0.20833333 0.50847458 0.5       ]
 [0.33333333 0.25       0.57627119 0.45833333]
 [0.5        0.41666667 0.61016949 0.54166667]
 [0.41666667 0.25       0.50847458 0.45833333]
 [0.19444444 0.125      0.38983051 0.375     ]
 [0.36111111 0.29166667 0.54237288 0.5       ]
 [0.38888889 0.41666667 0.54237288 0.45833333]
 [0.38888889 0.375      0.54237288 0.5       ]
 [0.52777778 0.375      0.55932203 0.5       ]
 [0.22222222 0.20833333 0.33898305 0.41666667]
 [0.38888889 0.33333333 0.52542373 0.5       ]
 [0.55555556 0.54166667 0.84745763 1.        ]
 [0.41666667 0.29166667 0.69491525 0.75      ]
 [0.77777778 0.41666667 0.83050847 0.83333333]
 [0.55555556 0.375      0.77966102 0.70833333]
 [0.61111111 0.41666667 0.81355932 0.875     ]
 [0.91666667 0.41666667 0.94915254 0.83333333]
 [0.16666667 0.20833333 0.59322034 0.66666667]
 [0.83333333 0.375      0.89830508 0.70833333]
 [0.66666667 0.20833333 0.81355932 0.70833333]
 [0.80555556 0.66666667 0.86440678 1.        ]
 [0.61111111 0.5        0.69491525 0.79166667]
 [0.58333333 0.29166667 0.72881356 0.75      ]
 [0.69444444 0.41666667 0.76271186 0.83333333]
 [0.38888889 0.20833333 0.6779661  0.79166667]
 [0.41666667 0.33333333 0.69491525 0.95833333]
 [0.58333333 0.5        0.72881356 0.91666667]
 [0.61111111 0.41666667 0.76271186 0.70833333]
 [0.94444444 0.75       0.96610169 0.875     ]
 [0.94444444 0.25       1.         0.91666667]
 [0.47222222 0.08333333 0.6779661  0.58333333]
 [0.72222222 0.5        0.79661017 0.91666667]
 [0.36111111 0.33333333 0.66101695 0.79166667]
 [0.94444444 0.33333333 0.96610169 0.79166667]
 [0.55555556 0.29166667 0.66101695 0.70833333]
 [0.66666667 0.54166667 0.79661017 0.83333333]
 [0.80555556 0.5        0.84745763 0.70833333]
 [0.52777778 0.33333333 0.6440678  0.70833333]
 [0.5        0.41666667 0.66101695 0.70833333]
 [0.58333333 0.33333333 0.77966102 0.83333333]
 [0.80555556 0.41666667 0.81355932 0.625     ]
 [0.86111111 0.33333333 0.86440678 0.75      ]
 [1.         0.75       0.91525424 0.79166667]
 [0.58333333 0.33333333 0.77966102 0.875     ]
 [0.55555556 0.33333333 0.69491525 0.58333333]
 [0.5        0.25       0.77966102 0.54166667]
 [0.94444444 0.41666667 0.86440678 0.91666667]
 [0.55555556 0.58333333 0.77966102 0.95833333]
 [0.58333333 0.45833333 0.76271186 0.70833333]
 [0.47222222 0.41666667 0.6440678  0.70833333]
 [0.72222222 0.45833333 0.74576271 0.83333333]
 [0.66666667 0.45833333 0.77966102 0.95833333]
 [0.72222222 0.45833333 0.69491525 0.91666667]
 [0.41666667 0.29166667 0.69491525 0.75      ]
 [0.69444444 0.5        0.83050847 0.91666667]
 [0.66666667 0.54166667 0.79661017 1.        ]
 [0.66666667 0.41666667 0.71186441 0.91666667]
 [0.55555556 0.20833333 0.6779661  0.75      ]
 [0.61111111 0.41666667 0.71186441 0.79166667]
 [0.52777778 0.58333333 0.74576271 0.91666667]
 [0.44444444 0.41666667 0.69491525 0.70833333]]
4.Read csv datafile using pandas dataframe then find the covariance and correlation
In [14]:
# for showing the coviariance
iris = pd.read_csv("iris.csv")
iris
Out[14]:
sepal.length	sepal.width	petal.length	petal.width	variety
0	5.1	3.5	1.4	0.2	Setosa
1	4.9	3.0	1.4	0.2	Setosa
2	4.7	3.2	1.3	0.2	Setosa
3	4.6	3.1	1.5	0.2	Setosa
4	5.0	3.6	1.4	0.2	Setosa
...	...	...	...	...	...
145	6.7	3.0	5.2	2.3	Virginica
146	6.3	2.5	5.0	1.9	Virginica
147	6.5	3.0	5.2	2.0	Virginica
148	6.2	3.4	5.4	2.3	Virginica
149	5.9	3.0	5.1	1.8	Virginica
150 rows × 5 columns

In [15]:
iris.cov()
Out[15]:
sepal.length	sepal.width	petal.length	petal.width
sepal.length	0.685694	-0.042434	1.274315	0.516271
sepal.width	-0.042434	0.189979	-0.329656	-0.121639
petal.length	1.274315	-0.329656	3.116278	1.295609
petal.width	0.516271	-0.121639	1.295609	0.581006
